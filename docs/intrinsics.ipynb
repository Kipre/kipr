{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrinsics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "### Subscript\n",
    "\n",
    "A subscript is a mapping from the dimensions (axes of the shape) to a collection of indexes along this axis.\n",
    "\n",
    "Let's say $n_d$ is the number of dimensions of an array $A$, and $d_0, d_1, ... d_{n_d}$ are the sizes along those dimensions. We can further define $F$, a filter as a sequence $(f_0, f_1, ... f_{n_d})$ where for any $i$, $f_i$ is the sequence of selected indexes along this axis. \n",
    "\n",
    "If $(i_0, i_1, ..., i_{n_d})$ are the indexes of an element in $A$ and $D$ is the (flat) buffer storing this data, then:\n",
    "\n",
    "$$A(i_0, i_1, ..., i_{n_d}) = D(\\sum_{k=0}^{n_d} i_k s_k)$$\n",
    "\n",
    "where the $s_i$ are the strides of $A$.\n",
    "\n",
    "$$\\forall i \\in \\{0, 1, ..., n_d - 1\\}, s_i = \\prod_{k=i+1}^{n_d} d_k \\text{ and } s_{n_d} = 1$$\n",
    "\n",
    "If we have $I = \\sum_{k=0}^{n_d} i_k s_k$ then if we want to find $I'$ for $i_{k_{new}}$ for example: we can do $I' = I + s_k(i_{k_{new}} - i_k)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape compatibility\n",
    "\n",
    "$(...,a, b) \\times (..., b, c) \\rightarrow (...,a, c)$\n",
    "\n",
    "not implemented $(...,a) \\times (..., 1, b) \\rightarrow (...,a, b)$\n",
    "\n",
    "not implemented $(...,a, b) \\times (..., b) \\rightarrow (...,a)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(u) = ln(u)$\n",
    "\n",
    "$f'(u) = \\frac{1}{u}$\n",
    "\n",
    "$f(v) = softmax(v) = \\frac{e^v}{reduce\\_sum(e^v)} = \\frac{e^v}{\\sum e^{v_i}}$\n",
    "$f(v)_i = softmax(v)_i = \\frac{e^{v_i}}{\\sum_j e^{v_j}}$\n",
    "\n",
    "$f'(v)_i = \\frac{e^{v_i} \\sum_j e^{v_j} - e^{2v_i}}{(\\sum_j e^{v_j})^2} = softmax(v)_i - softmax(v)_i^2$\n",
    "\n",
    "$f'(v) = softmax(v) - softmax(v)^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(y, u) = -\\sum y_i$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
